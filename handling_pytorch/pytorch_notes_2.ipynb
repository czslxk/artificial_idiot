{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch 基本用法（二）\n",
    "本节介绍 Pytorch 内部的自动求导机制。\n",
    "\n",
    "下面可能会把求导和求梯度两个词有一些混用，但是本质上是同一件事。\n",
    "\n",
    "## 计算图\n",
    "很多地方在讲解反向传播的时候都用**计算图**来演示传播的过程。\n",
    "\n",
    "![](pytorch_notes_assets/1.png)\n",
    "\n",
    "Pytorch 的自动微分机制差不多就是基于计算图实现的。每一个圆点代表 `Tensor` 对象，方点则代表 `Function` 对象。\n",
    "\n",
    "\n",
    "## 求梯度的开关\n",
    "Pytorch 非常厉害的一点就在于可以自动计算梯度。\n",
    "\n",
    "之前介绍的 `Tensor` 是 Pytorch 的核心类，如果将其属性 `.requires_grad` 设置为 `True`，它将开始追踪（或者说，记录）在其上的所有操作。使用 `.requires_grad_()` 方法也可以改变这个属性。\n",
    "\n",
    "一个张量是否被计算梯度和它所处的环境有关。如果一个输入需要计算梯度，那么输出就要计算梯度；反之如果所有输入都不需要梯度，那么输出就不需要梯度。\n",
    "\n",
    "**注意：如果处于追踪状态，那么一个计算图只有叶子节点可以停止被追踪，非叶节点则不可以。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.ones((2, 2), requires_grad=True)\n",
    "b = torch.eye(2)\n",
    "c = a + b\n",
    "print(c.requires_grad)\n",
    "a.requires_grad_(False)\n",
    "print(a.requires_grad)\n",
    "# c.requires_grad_(False)\n",
    "\"\"\"\n",
    "---------------------------------------------------------------------------\n",
    "RuntimeError                              Traceback (most recent call last)\n",
    "<ipython-input-26-8098baefffea> in <module>\n",
    "      6 a.requires_grad_(False)\n",
    "      7 print(a.requires_grad)\n",
    "----> 8 c.requires_grad_(False)\n",
    "\n",
    "RuntimeError: you can only change requires_grad flags of leaf variables. \n",
    "If you want to use a computed variable in a subgraph that doesn't require differentiation use var_no_grad = var.detach().\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果要让一个张量停止被追踪，可以调用 `.detach_()` 方法。如果调用不带下划线的就会返回该张量的一个**浅拷贝**，该拷贝不被记录梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[111.,   1.],\n",
      "        [  1.,   2.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "d = c.detach()\n",
    "d[0, 0] = 111\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在某些不需要梯度的环境下（如测试模型时）可以直接使用 `with torch.no_grad()` 将不想被追踪的操作代码块包裹起来。\n",
    "\n",
    "## 反向传播\n",
    "每个张量都有一个 `.grad_fn` 属性，该属性即创建该张量的 `Function`, 就是说该张量是不是通过某些运算得到的，若是，则 `grad_fn` 返回一个与这些运算相关的对象，否则是 `None`。\n",
    "\n",
    "（这里也从某种程度上体现了 Pytorch 的面向对象思想...？）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SumBackward0 object at 0x0000012A4B46C400>\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 4, requires_grad=True)\n",
    "y = torch.randn(3, 4)\n",
    "z = torch.randn(3, 4)\n",
    "a = x * y\n",
    "b = a + z\n",
    "c = torch.sum(b)\n",
    "print(c.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后调用 `.backward()` 方法就可以计算梯度，结果将**累积**到 `.grad` 属性中。注意之前用了**累积**一词，这意味着一个 `Tensor` 的梯度默认是累加的，不会自动清空。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9333, -0.2024, -0.0907,  1.4957],\n",
      "        [-0.0571, -0.7191, -0.0418,  1.0434],\n",
      "        [ 0.4795, -1.1440,  0.3909, -0.9394]])\n",
      "tensor([[-1.8666, -0.4048, -0.1814,  2.9914],\n",
      "        [-0.1142, -1.4383, -0.0837,  2.0868],\n",
      "        [ 0.9589, -2.2880,  0.7819, -1.8788]])\n"
     ]
    }
   ],
   "source": [
    "c.backward()\n",
    "print(x.grad)\n",
    "\n",
    "a = x * y\n",
    "b = a + z\n",
    "c = torch.sum(b)\n",
    "c.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里输出的就是 $c$ 关于 $x$ 的梯度（偏导数），即 $\\frac{\\partial c}{\\partial x}$。\n",
    "\n",
    "**注意：这里的 `.backward()` 方法不能被连续调用。**因为 Pytorch 计算梯度使用的是动态图机制，即每一次反向传播都会重新构建计算图，完成传播后销毁。如果要保留计算图需要指定 `retain_graph=True`，不过目前不需要这个。\n",
    "\n",
    "我们想要每次传播之后清空梯度，就可以使用 `.grad.data.zero_()` 实现梯度的清零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9333, -0.2024, -0.0907,  1.4957],\n",
      "        [-0.0571, -0.7191, -0.0418,  1.0434],\n",
      "        [ 0.4795, -1.1440,  0.3909, -0.9394]])\n"
     ]
    }
   ],
   "source": [
    "a = x * y\n",
    "b = a + z\n",
    "c = torch.sum(b)\n",
    "x.grad.data.zero_()\n",
    "c.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：这里是已经传播了一次的情况下，最开始 `.grad` 属性为 `None` 时不能这么做。**所以有时候需要特殊判断这种情况。\n",
    "\n",
    "这里演示的是简单的标量对张量求导，那么张量对张量怎么求呢？这是一个比较复杂的问题，暂且不讨论。"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}